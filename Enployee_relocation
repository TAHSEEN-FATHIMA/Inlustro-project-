import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Generate synthetic dataset
np.random.seed(42)
num_samples = 1000

data = pd.DataFrame({
    'EmployeeID': range(1, num_samples + 1),
    'Age': np.random.randint(22, 60, size=num_samples),
    'Department': np.random.choice(['Sales', 'HR', 'IT', 'Finance', 'Operations'], size=num_samples),
    'YearsAtCompany': np.random.randint(0, 20, size=num_samples),
    'PerformanceRating': np.random.choice([1, 2, 3, 4, 5], size=num_samples),
    'JobSatisfaction': np.random.choice([1, 2, 3, 4], size=num_samples),
    'WillingnessToRelocate': np.random.choice([0, 1], size=num_samples)
})

# Step 2: Generate meaningful RelocationSuitability based on logic
conditions = (
    (data['WillingnessToRelocate'] == 1) &
    (data['JobSatisfaction'] <= 2) &
    (data['PerformanceRating'] >= 3)
)
data['RelocationSuitability'] = np.where(conditions, 1, 0)

# Step 3: Encode categorical variables
label_encoder = LabelEncoder()
data['DepartmentEncoded'] = label_encoder.fit_transform(data['Department'])

# Step 4: Prepare features and target
features = ['Age', 'YearsAtCompany', 'PerformanceRating', 'JobSatisfaction', 'WillingnessToRelocate', 'DepartmentEncoded']
target = 'RelocationSuitability'

X = data[features]
y = data[target]

# Optional: Check class balance
print("Class Distribution:\n", y.value_counts(), "\n")

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 6: Grid Search for best Random Forest parameters
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)

# Step 7: Predict and evaluate
model = grid_search.best_estimator_
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nImproved Model Accuracy: {accuracy:.2f}\n")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Step 8: Feature importance plot
importances = model.feature_importances_
feature_names = X.columns

plt.figure(figsize=(10, 6))
plt.barh(feature_names, importances, color='skyblue')
plt.xlabel('Importance Score')
plt.title('Feature Importance for Employee Relocation Prediction')
plt.grid(True)
plt.tight_layout()
plt.show()





------------------
output
------------------

Class Distribution:
0    691
1    309

Best Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}

Improved Model Accuracy: 0.86

Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.90      0.91       139
           1       0.76      0.78      0.77        61

    accuracy                           0.86       200
   macro avg       0.83      0.84      0.84       200
weighted avg       0.86      0.86      0.86       200
Feature Importance for Employee Relocation Prediction
-------------------------------------------------------
| WillingnessToRelocate    | ██████████████████████████
| JobSatisfaction          | ████████████████████
| PerformanceRating        | ████████████
| DepartmentEncoded        | ██████
| YearsAtCompany           | ████
| Age                      | ███
